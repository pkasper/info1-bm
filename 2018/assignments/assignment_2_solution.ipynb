{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are native and numpy variants for some segments. Each segment expects a native input but may output numpy data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command Line Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Command Line Emulation\n",
    "sys.argv = [\"assignment_2.py\", \n",
    "            \"--input=sensors_graz.csv\", \n",
    "            \"--output=assignment_2.csv\",\n",
    "            \"--resampling_delta\", \"3600\",\n",
    "            \"--window_smoothing\", \"2592000\"]\n",
    "sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse args \n",
    "arg_parser = argparse.ArgumentParser(description=\"Load and clean a sensor data file\")\n",
    "arg_parser.add_argument(\"-i\", \"--input\", type=str, required=True, help=\"Path to the input file\")\n",
    "arg_parser.add_argument(\"-o\", \"--output\", type=str, required=True, help=\"Path to the output file\")\n",
    "arg_parser.add_argument(\"-s\", \"--sensors\", type=int, nargs=\"+\", default=[], help=\"Sensor IDs to process\")\n",
    "arg_parser.add_argument(\"-r\", \"--resampling_delta\", type=int, default=300, help=\"The timedelta for the resampling\")\n",
    "arg_parser.add_argument(\"-w\", \"--window_smoothing\", type=int, help=\"The timedelta for the smoothing\")\n",
    "\n",
    "cmd_args = arg_parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if input file exists\n",
    "if not os.path.isfile(cmd_args.input):\n",
    "    raise FileNotFoundError(\"Input file does not exist\")\n",
    "    \n",
    "if os.path.splitext(cmd_args.input)[1] != \".csv\":\n",
    "    raise NameError(\"Input file has to be a CSV (.csv ending)\")\n",
    "\n",
    "# Check if output directory exists\n",
    "output_dir, output_file = os.path.split(cmd_args.output)\n",
    "if not os.path.isdir(output_dir) and output_dir != \"\":\n",
    "    raise NotADirectoryError(\"Output directory does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Parsing the Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create data entries from lines\n",
    "\n",
    "def create_data_entry(headers, data):\n",
    "    new_entry = dict()\n",
    "    for key, value in zip(headers, data):\n",
    "        new_entry[key] = value\n",
    "    return new_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file \n",
    "\n",
    "with open(cmd_args.input, \"r\") as input_file:\n",
    "    # put all lines into a single list\n",
    "    lines = [l.strip() for l in input_file.readlines()]\n",
    "    # the first line contains the headers. so we cut that off\n",
    "    headers = lines[0].split(\",\")\n",
    "    # the data is anything below the header line\n",
    "    data_lines = lines[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the data in a dictionary. This is constructed as follows: The dict has one entry for each sensor (`sensor_id`) and the values contain the sensor id (again), location information (lat/lon) which type the sensor is and a dictionary for the data. In that data we have a list for timestamps and lists containing the values for each metric the sensor reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = dict()\n",
    "\n",
    "# read through each line\n",
    "for data_line in data_lines:\n",
    "    data_line = data_line.split(\",\")\n",
    "    entry = create_data_entry(headers, data_line)\n",
    "    sensor_id = int(entry[\"sensor_id\"])\n",
    "    timestamp = np.datetime64(entry['timestamp']) # convert this to a datetime object\n",
    "    p_1 = entry['P1']\n",
    "    p_2 = entry['P2']\n",
    "        \n",
    "    # check if the this is a sensor we want\n",
    "    if cmd_args.sensors == [] or sensor_id in cmd_args.sensors:\n",
    "        # If we do not have the sensor in the dict already, then create a new entry with the basic structure\n",
    "        if sensor_id not in sensors:\n",
    "            sensors[sensor_id] = dict()\n",
    "            sensors[sensor_id]['sensor_id'] = sensor_id\n",
    "            sensors[sensor_id]['lat'] = entry['lat']\n",
    "            sensors[sensor_id]['lon'] = entry['lon']\n",
    "            sensors[sensor_id]['sensor_type'] = entry['sensor_type']\n",
    "            sensors[sensor_id]['data'] = dict()\n",
    "            sensors[sensor_id]['data']['timestamp'] = []\n",
    "            sensors[sensor_id]['data']['P1'] = []\n",
    "            sensors[sensor_id]['data']['P2'] = []\n",
    "        \n",
    "        # here we can be sure the entry exists so we can just add the data\n",
    "        sensors[sensor_id]['data']['timestamp'].append(np.datetime64(timestamp)) # cast to a numpy \n",
    "        sensors[sensor_id]['data']['P1'].append(float(p_1)) # cast to a float\n",
    "        sensors[sensor_id]['data']['P2'].append(float(p_2)) # cast to a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a quick check to see if the data lists are all the same length. If not something strange went wrong...\n",
    "\n",
    "for sensor_id in sensors:\n",
    "    data_dict = sensors[sensor_id]['data']\n",
    "    \n",
    "    if len(data_dict['timestamp']) != len(data_dict['P1']) and len(data_dict['timestamp']) != len(data_dict['P2']):\n",
    "        raise ValueError(\"Something went wrong. The data lengths do not match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the data entries (by timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Pure python variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor_id in sensors:\n",
    "    data_dict = sensors[sensor_id]['data']\n",
    "    \n",
    "    value_indices = list(range(len(data_dict['timestamp'])))\n",
    "    sort_order = sorted(value_indices, key=data_dict['timestamp'].__getitem__)\n",
    "    # variant with a lambda. does exactly the same actually just doesnt use a hidden built in function\n",
    "    # sort_order = sorted(value_indices, key=lambda x: data_dict['timestamp'][x])\n",
    "    \n",
    "    # now let's rearrange the timestamps and value lists\n",
    "    for key in [\"timestamp\", \"P1\", \"P2\"]:\n",
    "        sensors[sensor_id]['data'][key] = [data_dict[key][x] for x in sort_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy variant"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for sensor_id in sensors:\n",
    "    data_dict = sensors[sensor_id]['data']\n",
    "    \n",
    "    # argsort returns the indices of the sorting algorithm\n",
    "    value_indices = np.argsort(data_dict['timestamp'])\n",
    "    \n",
    "    # by accessing data_array[order_array] we get the data_array in the order defined by the order array. \n",
    "    # Note that this only works for numpy arrays!\n",
    "    for key in [\"timestamp\", \"P1\", \"P2\"]:\n",
    "        sensors[sensor_id]['data'][key] = np.array(data_dict[key])[value_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the original values to a \"data_orig\" entry in the sensors dict.\n",
    "# Use a deepcopy as the objects in the data dict are mutable! Thus we use the copy package!\n",
    "\n",
    "for sensor_id in sensors:\n",
    "    sensors[sensor_id]['data_orig'] = copy.deepcopy(sensors[sensor_id]['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cut single outliers\n",
    "\n",
    "Outlier definition: A value is higher than twice the mean of the previous and next value. (> prev+next) <br />\n",
    "When we encounter an outlier we interpolate between the previous and the next. (Do this online)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_outlier(val, prev_val, next_val):\n",
    "    if val is None or prev_val is None or next_val is None:\n",
    "        return False\n",
    "    return val > (prev_val + next_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_smoothing_passes = 5\n",
    "for sensor_id in sensors:\n",
    "    for _ in range(num_smoothing_passes):\n",
    "        for key in [\"P1\", \"P2\"]:\n",
    "            data_list = sensors[sensor_id]['data'][key]\n",
    "            for current_index in range(1, len(data_list) - 1):\n",
    "                if is_outlier(data_list[current_index], data_list[current_index - 1], data_list[current_index + 1]):\n",
    "                    data_list[current_index] = (data_list[current_index - 1] + data_list[current_index + 1]) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resampling to the interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the median as smoothing function as it is less affected by outliers. alternative would be the mean\n",
    "\n",
    "def resampling_function(array):\n",
    "    \"\"\"\n",
    "        calculates the median of list of integers\n",
    "    \"\"\"\n",
    "    # quick check to see if all elements in the list are numeric (float or int)\n",
    "    if sum(isinstance(x, (int, float)) for x in array) != len(array):\n",
    "        print(array[0], type(array[0]))\n",
    "        raise TypeError(\"Expected a list of numerical values. Got at least one non-numerical\")\n",
    "    \n",
    "    sorted_array = sorted(array)\n",
    "    num_values = len(array)\n",
    "    median_index = (num_values - 1) // 2\n",
    "    median = None\n",
    "\n",
    "    if (num_values % 2):\n",
    "        median = sorted_array[median_index]\n",
    "    else:\n",
    "        median = (sorted_array[median_index] + sorted_array[median_index + 1])/2\n",
    "    return median\n",
    "    \n",
    "# numpy variant\n",
    "# resampling_function = np.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample the values. calculate the resampling function per window. use timestamp and timedelta datatypes. \n",
    "# start at 00:00 on the first day and end at 24:00 on the last. \n",
    "# center the resampling function around the current interval\n",
    "# source the data from the data_orig object and store it into data.\n",
    "\n",
    "# 30 minutes = 30 * 60 seconds\n",
    "\n",
    "resampling_delta = np.timedelta64(cmd_args.resampling_delta, \"s\")\n",
    "\n",
    "for sensor_id in sensors:\n",
    "    data_dict = sensors[sensor_id]['data_orig']\n",
    "    current_timestamp = np.datetime64(np.datetime64(data_dict['timestamp'][0], \"D\"), \"s\")\n",
    "    last_timestamp = np.datetime64(np.datetime64(data_dict['timestamp'][-1], \"D\") + 1, \"s\")\n",
    "    \n",
    "    # create a new (empty) data dict. we put the smoothed values in there\n",
    "    # and then overwrite the entire data object at the end\n",
    "    data_dict_new = {\"timestamp\": [],\n",
    "                     \"P1\": [], \n",
    "                     \"P2\": []}\n",
    "    \n",
    "    # since we have sorted the data by timestamp we can carry an index indicating where we are in the list.\n",
    "    # this stops us from always having to iterate over the entire list\n",
    "    current_index = 0\n",
    "    \n",
    "    # do as long as we have not reached the ending timestamp\n",
    "    while current_timestamp <= last_timestamp:\n",
    "        # the indices of the values within the current windown. we want the indices \n",
    "        # to smooth the values for both data lists (P1 and P2)!\n",
    "        value_indices = []\n",
    "        # do forever (or until we reach the end). we manually break out of the loop\n",
    "        while current_index < len(data_dict['timestamp']):\n",
    "            # break out of the loop when the timestamp of the current index is larger than \n",
    "            # the current timestamp plus half the resampling delta (since we center around them)\n",
    "            if data_dict['timestamp'][current_index] >= current_timestamp + (resampling_delta / 2):\n",
    "                break\n",
    "            # Add the index to the window\n",
    "            value_indices.append(current_index)\n",
    "            \n",
    "            current_index += 1\n",
    "        \n",
    "        # now we calculate the smoothed values using the smoothing function and add this to the new data object\n",
    "        data_dict_new['timestamp'].append(current_timestamp)\n",
    "        for key in [\"P1\", \"P2\"]:\n",
    "            # the window could be empty (sensor could have been offline). in that case just add \"None\"\n",
    "            if len(value_indices) == 0:\n",
    "                 data_dict_new[key].append(None)\n",
    "            else:\n",
    "                data_dict_new[key].append(resampling_function([data_dict[key][x] for x in value_indices]))\n",
    "            \n",
    "        #now that we have finished this timestamp move on to the next\n",
    "        current_timestamp += resampling_delta\n",
    "        \n",
    "\n",
    "    # overwrite the data object (do not touch the data_orig)\n",
    "    sensors[sensor_id]['data'] = data_dict_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rolling Means\n",
    "This smoothes the whole data over a longer duration"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# we have precise timestamps for our data. so we can just divide the resampling interval by the smoothing window\n",
    "if cmd_args.window_smoothing is not None:\n",
    "    window_size = int((np.timedelta64(cmd_args.window_smoothing, \"s\") / 2) / resampling_delta)\n",
    "    \n",
    "    for sensor_id in sensors:\n",
    "        for key in [\"P1\", \"P2\"]:\n",
    "            data_list = sensors[sensor_id]['data'][key]\n",
    "            data_list_new = copy.deepcopy(data_list)\n",
    "            \n",
    "            for current_index in range(window_size, len(data_list) - window_size):\n",
    "                i_from = current_index-window_size\n",
    "                i_to = current_index+window_size+1\n",
    "                valid_list = [x for x in data_list[i_from:i_to] if x is not None]\n",
    "                data_list_new[current_index] = sum(valid_list) / max(len(valid_list), 1)\n",
    "                \n",
    "            sensors[sensor_id]['data'][key] = data_list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this variant takes edges into account\n",
    "\n",
    "if cmd_args.window_smoothing is not None:\n",
    "    window_size = int((np.timedelta64(cmd_args.window_smoothing, \"s\") / 2) / resampling_delta)\n",
    "    \n",
    "    for sensor_id in sensors:\n",
    "        for key in [\"P1\", \"P2\"]:\n",
    "            data_list = sensors[sensor_id]['data'][key]\n",
    "            data_list_new = copy.deepcopy(data_list)\n",
    "            \n",
    "            for current_index in range(len(data_list)):\n",
    "                i_from = max(current_index-window_size, 0)\n",
    "                i_to = min(current_index+window_size+1, len(data_list))\n",
    "                valid_list = [x for x in data_list[i_from:i_to] if x is not None]\n",
    "                data_list_new[current_index] = sum(valid_list) / max(len(valid_list), 1)\n",
    "                \n",
    "            sensors[sensor_id]['data'][key] = data_list_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "This functionality below is provided with the assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ass_2_plot import plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor_id, sensor_data in sensors.items():\n",
    "    plot_data(\"Sensor ID: {id}\".format(id=sensor_id), \n",
    "              sensor_data['data_orig'], \n",
    "              sensor_data['data'], \n",
    "              filename=\"{outname}_{id}.png\".format(outname=os.path.basename(cmd_args.output), id=sensor_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"location\" in headers:\n",
    "    headers.remove(\"location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_lines = []\n",
    "for sensor_id, sensor in sensors.items():\n",
    "    data = sensor['data']\n",
    "    for ts, p1, p2 in zip(data['timestamp'], data['P1'], data['P2']):\n",
    "        ts = str(ts).replace(\"T\", \" \")\n",
    "        if p1 == None:\n",
    "            p1 = \"NaN\"\n",
    "        else:\n",
    "            p1 = np.round(p1, 3)\n",
    "        if p2 == None:\n",
    "            p2 = \"NaN\"\n",
    "        else:\n",
    "            p2 = np.round(p2, 3)\n",
    "        line = [sensor_id, sensor['sensor_type'], sensor['lat'], sensor['lon'], ts, p1, p2]\n",
    "        dump_lines.append(line)\n",
    "\n",
    "dump_lines.sort(key=lambda x: (x[4], x[0]))\n",
    "dump_lines = [','.join((str(y) for y in x)) for x in dump_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cmd_args.output, \"w\") as output_file:\n",
    "    output_file.write(\",\".join(headers))\n",
    "    output_file.write(\"\\n\")\n",
    "    for line in dump_lines:\n",
    "        output_file.write(line)\n",
    "        output_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.7 - Patrick",
   "language": "python",
   "name": "patrick"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
