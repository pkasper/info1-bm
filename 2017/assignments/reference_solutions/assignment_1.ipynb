{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Informatics 1 for Biomedical Engineering</center>  \n",
    "## <center>Assignment 1 - Reference Solution</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Comment Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Author:      Patrick Kasper\n",
    "# MatNr:       0730294\n",
    "# Description: Tasks 1\n",
    "# Comments:    Assignment 1 Reference.\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Command Line Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1) Detect if code is run in a ipython notebook. If so, emulate the argv. (NOTE: Not required in Task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ipykernel' in sys.modules:\n",
    "    sys.argv = [\"assignment_1.py\", \"DATA/RECORDS\"] # change to whatever you want/need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.2) Check values and set defaults if neccessairy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if len(sys.argv) == 2:\n",
    "    data_dir = os.path.dirname(sys.argv[1]) + os.path.sep\n",
    "    records_filename = os.path.basename(sys.argv[1])\n",
    "else:\n",
    "    data_dir = \"DATA\" + os.path.sep\n",
    "    records_filename = \"RECORDS\"\n",
    "output_dir = os.path.sep\n",
    "filetypes = [\".hea\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.3) Check if the records file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir + records_filename):\n",
    "    print(\"[ERROR]  Records file not found...\")\n",
    "    exit() # This causes the kernel to die if used in a notebook..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.4) Read the entries in the records file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + records_filename, \"r\") as records_file: # This way we do not need to worry about closing the file\n",
    "    # get the list of lines and strip surrounding whitespaces and line breaks\n",
    "    expected_records = [f.strip() for f in records_file.readlines()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record: drive01 ... OK\n",
      "Record: drive02 ... OK\n",
      "Record: drive03 ... OK\n",
      "Record: drive04 ... OK\n",
      "Record: drive05 ... OK\n",
      "Record: drive06 ... OK\n",
      "Record: drive07 ... OK\n",
      "Record: drive08 ... OK\n",
      "Record: drive09 ... OK\n",
      "Record: drive10 ... OK\n",
      "Record: drive11 ... OK\n",
      "Record: drive12 ... OK\n",
      "Record: drive13 ... OK\n",
      "Record: drive14 ... OK\n",
      "Record: drive15 ... OK\n",
      "Record: drive16 ... OK\n",
      "Record: drive17a ... OK\n",
      "Record: drive17b ... OK\n"
     ]
    }
   ],
   "source": [
    "found_records = []\n",
    "for record in expected_records:\n",
    "    #this would check for multiple filetypes too...\n",
    "    if sum([os.path.isfile(data_dir + record + ext) for ext in filetypes]) == len(filetypes): \n",
    "        print(\"Record: {record_name} ... OK\".format(record_name=record))\n",
    "        found_records.append(record)\n",
    "    else: \n",
    "        print(\"Record: {record_name} ... NOT FOUND - skipping\".format(record_name=record))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Build the Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize it\n",
    "parsed_records = dict()\n",
    "\n",
    "# loop for all found records\n",
    "for record in found_records:\n",
    "    with open(data_dir + record + \".hea\", \"r\") as header_file:\n",
    "        # read all the lines and strio whitespaces and linebreaks\n",
    "        record_header = [x.strip() for x in header_file.readlines()]\n",
    "        # take the first line (which contains general information) and remove it from the list\n",
    "        record_line = record_header.pop(0)\n",
    "        #split by whitespaces and unpack the variables\n",
    "        record_name, number_of_signals, sampling_frequency, number_of_samples = record_line.split(\" \")\n",
    "        \n",
    "        # create the first layer of the dictionary entry per record\n",
    "        parsed_records[record] = {\n",
    "            \"record_name\": record_name.lower(),\n",
    "            \"num_signals\": int(number_of_signals),\n",
    "            \"frequency\": float(sampling_frequency),\n",
    "            \"num_samples\": int(number_of_samples),\n",
    "            \"signals\": dict()\n",
    "        }\n",
    "        \n",
    "        # loop through the number of signals\n",
    "        # (if you just look at the remaining lines present you might run into trouble)\n",
    "        for signal_index in range(int(number_of_signals)):\n",
    "            signal_data = record_header[signal_index].split(\" \")\n",
    "            file_name = signal_data[0]\n",
    "            data_format_block = signal_data[1]\n",
    "            \n",
    "            # look for the x in the data_format block to see if we have multiple samples per frame\n",
    "            data_format_split_index = data_format_block.find(\"x\")\n",
    "            if data_format_split_index != -1:\n",
    "                data_format = int(data_format_block[:data_format_split_index])\n",
    "                samples_per_frame = int(data_format_block[data_format_split_index + 1 : ])\n",
    "            else:\n",
    "                data_format = int(data_format_block)\n",
    "                samples_per_frame = 1\n",
    "            unit = signal_data[2]\n",
    "            adc_resolution = int(signal_data[3])\n",
    "            adc_zero = int(signal_data[4])\n",
    "            initial_value = int(signal_data[5])\n",
    "            checksum = int(signal_data[6])\n",
    "            block_size = int(signal_data[7])\n",
    "            description = \" \".join(signal_data[8:]).lower() # take all the remaining text as record names\n",
    "            \n",
    "            # build the dictionary for each signal\n",
    "            signal_data = {\n",
    "                \"file_name\": file_name,\n",
    "                \"data_format\": data_format,\n",
    "                \"samples_per_frame\": samples_per_frame,\n",
    "                \"unit\": unit,\n",
    "                \"adc_resolution\": adc_resolution,\n",
    "                \"adc_zero\": adc_zero,\n",
    "                \"initial_value\": initial_value,\n",
    "                \"checksum\": checksum,\n",
    "                \"block_size\": block_size\n",
    "            }\n",
    "            \n",
    "            # add the signal dictionary to the signals key in the records dictionary\n",
    "            parsed_records[record][\"signals\"][description] = signal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Print Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.1) Gather the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defaultdicts allow us to create dictionaries with default values\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signal_dict = defaultdict(list)\n",
    "\n",
    "# loop though the key, value pairs in all parsed records. Key is the name and the value is the whole dictionary per signal\n",
    "# in signal dict we have the signals as key and a list of records containing said signal as value\n",
    "for record_name, record_data in parsed_records.items():\n",
    "    for signal in record_data[\"signals\"]:\n",
    "        signal_dict[signal].append(record_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.1) Print the output box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################\n",
      "# SIGNALS:\n",
      "#   ecg        18 record(s), missing: \n",
      "#   emg        15 record(s), missing: drive03, drive04, drive02\n",
      "#   foot gsr   18 record(s), missing: \n",
      "#   hand gsr   16 record(s), missing: drive13, drive02\n",
      "#   hr         16 record(s), missing: drive03, drive14\n",
      "#   marker     16 record(s), missing: drive01, drive03\n",
      "#   resp       18 record(s), missing: \n",
      "########################################################################\n"
     ]
    }
   ],
   "source": [
    "print(\"#\"*72) # box header\n",
    "print(\"# SIGNALS:\")\n",
    "#calculate the padding for the entries\n",
    "print_offset = max([len(x) for x in signal_dict.keys()]) + 3 \n",
    "\n",
    "# loop through the sorted signal dictionary\n",
    "for signal_name, signal_population in sorted(signal_dict.items()):\n",
    "    # use sets to calculate which records do not contain the key\n",
    "    missing = set(parsed_records.keys() - set(signal_population))\n",
    "    # prepare the format string (this is mainly so we keep the line length in check)\n",
    "    output_line = \"#   {signal_name:{print_offset}}{num_records:2} record(s), missing: {missing_records}\"\n",
    "    print(output_line.format(print_offset=print_offset, \n",
    "                             signal_name=signal_name,\n",
    "                             num_records=len(signal_population),\n",
    "                             missing_records= \", \".join(sorted(missing)))\n",
    "         )\n",
    "print(\"#\"*72) #box footer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Store Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"ass_1.p\", \"wb\") as dump_file:\n",
    "    pickle.dump(parsed_records, dump_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
