{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Author:      Patrick Kasper\n",
    "# MatNr:       0730294\n",
    "# Description: Assignment 2 Reference.\n",
    "# Comments: This is one possible solution for all the tasks \n",
    "#           required in assignment 2\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import struct\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    if str(type(get_ipython())) == \\\n",
    "       \"<class 'ipykernel.zmqshell.ZMQInteractiveShell'>\":\n",
    "            \n",
    "        print(\"Running from within a notebook. Overriding sys.argv\")\n",
    "        sys.argv = [\n",
    "            \"assignment_2.py\",\n",
    "            \"--data\",\n",
    "            \"DATA_MOD/\",\n",
    "            \"--verify\",\n",
    "            \"--in\",\n",
    "            \"DATA_MOD/ass_1.p\",\n",
    "            \"--gsr\"\n",
    "        ]\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(_path):\n",
    "    # check if file exists\n",
    "    if not os.path.isfile(_path):\n",
    "        print(\"[ERROR] Input pickle not found.\\nPath: {path}\"\n",
    "              .format(path=_path))\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        with open(_path, \"rb\") as input_pickle:\n",
    "            data = pickle.load(input_pickle)\n",
    "    except: # just catch em all for now\n",
    "        print(\"[ERROR] Loading input pickle failed\")\n",
    "        return False\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_read_binary(_data, _binary_dir):\n",
    "    signed_short_flag = \"h\"\n",
    "    unsigned_short_flag = \"H\"\n",
    "    \n",
    "    for record_name in sorted(_data.keys()):\n",
    "        data_files = set([data[record_name]['signals'][x]['file_name'] \n",
    "                          for x in data[record_name]['signals']])\n",
    "        signal_order = sorted(data[record_name][\"signals\"].keys())\n",
    "        for data_file in data_files:\n",
    "            # this is a bit more complex than needed \n",
    "            # because we only have 1 data file per record\n",
    "            signals = sorted([x for x in data[record_name]['signals'] \n",
    "                              if data[record_name]['signals'] \\\n",
    "                                     [x]['file_name'] \n",
    "                              == data_file]) \n",
    "            for signal_name in signals:\n",
    "                # fill it initially\n",
    "                data[record_name]['signals'][signal_name]['data'] = []\n",
    "                \n",
    "            if not os.path.isfile(_binary_dir + data_file):\n",
    "                continue\n",
    "                \n",
    "            with open(_binary_dir + data_file, \"rb\") as binary_data:\n",
    "                for _ in range(data[record_name]['num_samples']):\n",
    "                    for signal_name in signals:\n",
    "                        num_samples = data[record_name]['signals'] \\\n",
    "                                          [signal_name]['samples_per_frame']\n",
    "                        unpack_format = signed_short_flag * num_samples\n",
    "                        signal_values = struct.unpack(unpack_format, \n",
    "                                                      binary_data.read(\n",
    "                                                          num_samples * 2))\n",
    "                        _data[record_name]['signals'] \\\n",
    "                             [signal_name]['data'].append(signal_values)\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sublists(_list, _func):\n",
    "    return [_func(x) for x in _list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_checksum(_list):\n",
    "    unsigned_sum = sum(_list) & 0xFFFF\n",
    "    signed_sum = struct.unpack(\"h\",struct.pack(\"H\", unsigned_sum))[0]\n",
    "    \n",
    "    return signed_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_verify(_data):\n",
    "    flatten_function = lambda x: sum(x)\n",
    "    for record_name in sorted(_data.keys()):\n",
    "        for signal_name, signal_info \\\n",
    "        in _data[record_name]['signals'].items():\n",
    "        \n",
    "            checksum = calculate_checksum(\n",
    "                merge_sublists(signal_info['data'], flatten_function))\n",
    "            if checksum != signal_info['checksum']:\n",
    "                print(\"CHECKSUM FAILED {record} {signal}\"\n",
    "                      .format(record=record_name, \n",
    "                              signal=signal_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_correlate_gsr(_data):\n",
    "    target_signals = [\"hand gsr\", \"foot gsr\"]\n",
    "    for record_name in sorted(data.keys()):\n",
    "        if sum([x in _data[record_name][\"signals\"] \n",
    "                for x in target_signals]) == len(target_signals):\n",
    "            flatten_function = lambda x: sum(x)/len(x)\n",
    "            \n",
    "            first = merge_sublists(_data[record_name]\n",
    "                                   [\"signals\"]\n",
    "                                   [target_signals[0]]\n",
    "                                   ['data'], \n",
    "                                   flatten_function)\n",
    "            second = merge_sublists(_data[record_name]\n",
    "                                    [\"signals\"]\n",
    "                                    [target_signals[1]]\n",
    "                                    ['data'], \n",
    "                                    flatten_function)            \n",
    "            correlation_coefficient = pearsonr(first, second)\n",
    "\n",
    "            print(\"GSR {record} {coefficient:.4f}\"\n",
    "                  .format(record=record_name, \n",
    "                          coefficient=correlation_coefficient[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_store(_data, _path):\n",
    "    with open(_path, \"wb\") as output_pickle:\n",
    "        pickle.dump(_data, output_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args_parser = argparse.ArgumentParser()\n",
    "    args_parser.add_argument(\"-i\", \"--in\", \n",
    "                             type=str, \n",
    "                             default=\"ass_1.p\")\n",
    "    args_parser.add_argument('-o', \"--out\", \n",
    "                             type=str, \n",
    "                             default=\"ass_2.p\")\n",
    "    args_parser.add_argument('-d', \"--data\", \n",
    "                             type=str, \n",
    "                             default=\"DATA\")\n",
    "    args_parser.add_argument('--verify', \n",
    "                             action='store_true')\n",
    "    args_parser.add_argument('--gsr', \n",
    "                             action='store_true')\n",
    "    \n",
    "    args = vars(args_parser.parse_args())\n",
    "    \n",
    "    # fix the path if we need to\n",
    "    # check with list for windows systems (as they allow both)\n",
    "    if args['data'][-1] not in [os.path.sep, \"/\"]: \n",
    "        args['data'] += os.path.sep\n",
    "    \n",
    "    data = data_load(args['in'])\n",
    "    if data is False:\n",
    "        exit()\n",
    "    data = data_read_binary(data, args['data'])\n",
    "    \n",
    "    if args[\"verify\"]:\n",
    "        data_verify(data)\n",
    "        \n",
    "    if args[\"gsr\"]:\n",
    "        data_correlate_gsr(data)\n",
    "        \n",
    "    data_store(data, args['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
